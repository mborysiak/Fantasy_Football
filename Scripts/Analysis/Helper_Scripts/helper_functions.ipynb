{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==========\n",
    "# Dictionary for position relevant metrics\n",
    "#==========\n",
    "\n",
    "# initialize full position dictionary\n",
    "pos = {}\n",
    "\n",
    "#---------\n",
    "# RB dictionary\n",
    "#---------\n",
    " \n",
    "# initilize RB dictionary\n",
    "pos['RB'] = {}\n",
    "\n",
    "# total touch filter name\n",
    "pos['RB']['touch_filter'] = 'total_touches'\n",
    "\n",
    "# median feature categories\n",
    "pos['RB']['med_features'] = ['fp', 'tgt', 'receptions', 'total_touches', 'rush_yds', 'rec_yds', \n",
    "                   'rush_yd_per_game', 'rec_yd_per_game', 'rush_td', 'games_started', \n",
    "                   'qb_rating', 'qb_yds', 'pass_off', 'tm_rush_td', 'tm_rush_yds', \n",
    "                   'tm_rush_att', 'adjust_line_yds', 'ms_rush_yd', 'ms_rec_yd', 'ms_rush_td',\n",
    "                   'avg_pick', 'fp_per_touch', 'team_rush_avg_att']\n",
    "\n",
    "# sum feature categories\n",
    "pos['RB']['sum_features'] = ['total_touches', 'att', 'scrimmage_yds']\n",
    "\n",
    "# max feature categories\n",
    "pos['RB']['max_features'] = ['fp', 'rush_td', 'tgt', 'rush_yds', 'rec_yds', 'scrimmage_yds']\n",
    "\n",
    "# age feature categories\n",
    "pos['RB']['age_features'] = ['fp', 'rush_yd_per_game', 'rec_yd_per_game', 'total_touches', 'receptions', 'tgt',\n",
    "                             'ms_rush_yd', 'ms_rec_yd', 'available_rush_att', 'available_tgt', 'total_touches_sum',\n",
    "                             'scrimmage_yds_sum', 'avg_pick', 'fp_per_touch', 'ms_rush_yd_per_att', 'ms_tgts']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_target(df, year_start, year_end, median_features, sum_features, max_features, \n",
    "                    age_features, target_feature):\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    years = range(year_start+1, year_end+1)\n",
    "\n",
    "    for year in years:\n",
    "        \n",
    "        # adding the median features\n",
    "        past = df[df['year'] <= year]\n",
    "        for metric in median_features:\n",
    "            past = past.join(past.groupby('player')[metric].median(),on='player', rsuffix='_median')\n",
    "\n",
    "        for metric in max_features:\n",
    "            past = past.join(past.groupby('player')[metric].max(),on='player', rsuffix='_max')\n",
    "            \n",
    "        for metric in sum_features:\n",
    "            past = past.join(past.groupby('player')[metric].sum(),on='player', rsuffix='_sum')\n",
    "            \n",
    "        # adding the age features\n",
    "        suffix = '/ age'\n",
    "        for feature in age_features:\n",
    "            feature_label = ' '.join([feature, suffix])\n",
    "            past[feature_label] = past[feature] / past['age']\n",
    "        \n",
    "        # adding the values for target feature\n",
    "        year_n = past[past[\"year\"] == year]\n",
    "        year_n_plus_one = df[df['year'] == year+1][['player', target_feature]].rename(columns={target_feature: 'y_act'})\n",
    "        year_n = pd.merge(year_n, year_n_plus_one, how='left', left_on='player', right_on='player')\n",
    "        new_df = new_df.append(year_n)\n",
    "    \n",
    "    # creating dataframes to export\n",
    "    new_df = new_df.sort_values(by=['year', 'fp'], ascending=[False, False])\n",
    "    new_df = pd.concat([new_df, pd.get_dummies(new_df.year)], axis=1)\n",
    "    \n",
    "    df_train = new_df[new_df.year < year_end].reset_index(drop=True)\n",
    "    df_predict = new_df[new_df.year == year_end].drop('y_act', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    df_train['year'] = df_train.year.astype('int')\n",
    "    df_train = df_train.sort_values(['year', 'fp_per_game'], ascending=True).reset_index(drop=True)\n",
    "\n",
    "    df_predict['year'] = df_predict.year.astype('int')\n",
    "    \n",
    "    df_predict['year'] = df_predict.year - df_train.year.min()\n",
    "    df_train['year'] = df_train.year - df_train.year.min()\n",
    "    \n",
    "    return df_train, df_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(df_train):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    from my_plot import PrettyPlot\n",
    "    \n",
    "    plt.figure(figsize=(17,12))\n",
    "    k = 25\n",
    "    corrmat = abs(df_train.corr())\n",
    "    cols_large = corrmat.nlargest(k, 'y_act').index\n",
    "    hm_large = corrmat.nlargest(k,'y_act')[cols_large]\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns_plot = sns.heatmap(hm_large, cmap=\"YlGnBu\", cbar=True, annot=True, square=False, fmt='.2f', \n",
    "                 annot_kws={'size': 12});\n",
    "\n",
    "    fig = sns_plot.get_figure();\n",
    "    PrettyPlot(plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============\n",
    "# Create parameter dictionaries for each algorithm\n",
    "#=============\n",
    "\n",
    "lgbm_params = {\n",
    "    'n_estimators':[20, 25, 30, 35, 40, 50, 60],\n",
    "    'max_depth':[1, 2, 3],\n",
    "    'freature_fraction':[0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'min_child_weight': [5, 10, 15, 20, 25]\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': [20, 25, 30, 35, 40, 50, 60, 75], \n",
    "    'max_depth': [1, 2, 3], \n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1],\n",
    "    'min_child_weight': [10, 15, 20, 25, 30],\n",
    "    'freature_fraction':[0.6, 0.7, 0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': [10, 25, 50], \n",
    "    'depth': [1, 2, 3, 4, 5, 10]\n",
    "}\n",
    "\n",
    "ridge_params = {\n",
    "    'alpha': [50, 100, 150, 200, 250, 300, 400, 500]\n",
    "}\n",
    "\n",
    "lasso_params = {\n",
    "    'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2]\n",
    "}\n",
    "\n",
    "lasso_pca_params = {\n",
    "    'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2]\n",
    "}\n",
    "\n",
    "lr_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(name, params, rand=True, random_state=None):\n",
    "    \n",
    "    import random\n",
    "    from numpy import random\n",
    "    from xgboost import XGBRegressor\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.linear_model import Ridge\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from catboost import CatBoostRegressor\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    state = RandomState(random_state)\n",
    "    \n",
    "    rnd_params = {}\n",
    "    tmp_params = params[name]\n",
    "    if rand == True:\n",
    "        for line in tmp_params.items():\n",
    "            rnd_params[line[0]] = state.choice(line[1])\n",
    "    else:\n",
    "        rnd_params = tmp_params\n",
    "    \n",
    "    if name == 'lgbm':\n",
    "        estimator = LGBMRegressor(random_state=1, **rnd_params, min_data=1)\n",
    "        \n",
    "    if name == 'xgb':\n",
    "        estimator = XGBRegressor(random_state=1, **rnd_params)\n",
    "        \n",
    "    if name == 'ridge':\n",
    "        estimator = Ridge(random_state=1, **rnd_params)\n",
    "        \n",
    "    if name == 'lasso':\n",
    "        estimator = Lasso(random_state=1, **rnd_params)\n",
    "        \n",
    "    if name == 'catboost':\n",
    "        estimator = CatBoostRegressor(random_state=1, logging_level='Silent', **rnd_params)\n",
    "        \n",
    "    if name == 'lasso_pca':\n",
    "        estimator = Lasso(random_state=1, **rnd_params)\n",
    "        \n",
    "    if name == 'lr_pca':\n",
    "        estimator = LinearRegression()\n",
    "\n",
    "    return estimator, rnd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_split(train, val, scale=True, pca=False):\n",
    "    '''\n",
    "    input: train and validation or test datasets\n",
    "    output: datasets split into X features and y response for train / validation or test\n",
    "    '''\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    X_train = train.select_dtypes(include=['float', 'int', 'uint8']).drop('y_act', axis=1)\n",
    "    y_train = train.y_act\n",
    "    \n",
    "    try:    \n",
    "        X_val = val.select_dtypes(include=['float', 'int', 'uint8']).drop('y_act', axis=1)\n",
    "        y_val = val.y_act\n",
    "    except:\n",
    "        X_val = val.select_dtypes(include=['float', 'int', 'uint8'])\n",
    "        y_val = None\n",
    "    \n",
    "    if scale == True:\n",
    "        X_train = StandardScaler().fit_transform(X_train)\n",
    "        X_val = StandardScaler().fit_transform(X_val)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    if pca == True:\n",
    "        pca = PCA(n_components=10)\n",
    "        pca.fit(X_train)\n",
    "        \n",
    "        X_train = pca.transform(X_train)\n",
    "        X_val = pca.transform(X_val)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_residuals(estimator, X_train, y_train, X_val, y_val, train_error, val_error):\n",
    "    '''\n",
    "    input: estimator, feature set to be predicted, ground truth\n",
    "    output: sum of residuals for train and validation predictions\n",
    "    '''\n",
    "    predict_train = estimator.predict(X_train)\n",
    "    train_error.append(np.sum((predict_train-y_train)**2))\n",
    "    \n",
    "    predict_val = estimator.predict(X_val)\n",
    "    val_error.append(np.sum(abs(predict_val-y_val)**2))\n",
    "    \n",
    "    return train_error, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_compare(df, year_cutoff = 1):\n",
    "    \n",
    "    from scipy.stats import pearsonr\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df = df[df.year > year_cutoff].reset_index(drop=True)\n",
    "\n",
    "    lr = LinearRegression().fit(df.pred.values.reshape(-1,1), df.y_act)\n",
    "    r_sq_pred = round(lr.score(df.pred.values.reshape(-1,1), df.y_act), 3)\n",
    "    corr_pred = round(pearsonr(df.pred, df.y_act)[0], 3)\n",
    "    \n",
    "    lr = LinearRegression().fit(df.avg_pick.values.reshape(-1,1), df.y_act)\n",
    "    r_sq_avg_pick = lr.score(df.avg_pick.values.reshape(-1,1), df.y_act)\n",
    "    corr_avg_pick = round(pearsonr(df.avg_pick, df.y_act)[0], 3)\n",
    "\n",
    "    return [r_sq_pred, corr_pred, r_sq_avg_pick, corr_avg_pick]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rmse(estimator, X_train, y_train, X_val, y_val, train_rmses, val_rmses):\n",
    "    '''\n",
    "    input: estimator, feature set to be predicted, ground truth\n",
    "    output: RMSE value for out-of-sample predctions and list of predictions\n",
    "    '''\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "        \n",
    "    predict_train = estimator.predict(X_train)\n",
    "    train_rmses.append(np.sqrt(mean_squared_error(predict_train, y_train)))\n",
    "    \n",
    "    predict_val = estimator.predict(X_val)\n",
    "    val_rmses.append(np.sqrt(mean_squared_error(predict_val, y_val)))\n",
    "    \n",
    "    return train_rmses, val_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(estimators, params, df_train, iterations=50):\n",
    "    '''\n",
    "    input: training dataset, estimator\n",
    "    output: out-of-sample errors and predictions for 5 timeframes\n",
    "    '''\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    \n",
    "    param_tracker = {}\n",
    "    summary = pd.DataFrame()\n",
    "    for i in range(0, iterations):\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(str(datetime.datetime.now())[:-7])\n",
    "            print('Completed ' + str(i+1) + '/' + str(iterations) + ' iterations')\n",
    "            \n",
    "        # create empty dictionary for current iteration storage\n",
    "        param_tracker[i] = {}\n",
    "        \n",
    "        # create empty lists to store predictions and errors for each estimator\n",
    "        est_predictions=pd.DataFrame()\n",
    "        est_errors=pd.DataFrame()\n",
    "        for est in estimators:\n",
    "\n",
    "            # grab estimator and random parameters for estimator type\n",
    "            estimator, param_tracker[i][est] = get_estimator(est, params)\n",
    "        \n",
    "            # run through all years for given estimator\n",
    "            val_error = []    \n",
    "            train_error = [] \n",
    "            val_predictions = np.array([]) \n",
    "            years = df_train.year.unique()[1:]\n",
    "            \n",
    "            for m in years:\n",
    "                \n",
    "                # create training set for all previous years and validation set for current year\n",
    "                train_split = df_train[df_train.year < m]\n",
    "                val_split = df_train[df_train.year == m]\n",
    "        \n",
    "                # splitting in X and y\n",
    "                scale=False\n",
    "                if est == 'ridge':\n",
    "                    scale = True\n",
    "                if est == 'lasso': \n",
    "                    scale = True\n",
    "                    \n",
    "                pca=False\n",
    "                if est == 'lr_pca':\n",
    "                    pca=True\n",
    "                if est == 'lasso_pca':\n",
    "                    pca=True\n",
    "                    \n",
    "                X_train, X_val, y_train, y_val = X_y_split(train_split, val_split, scale, pca)\n",
    "        \n",
    "                # fit training data and predict validation data\n",
    "                estimator.fit(X_train, y_train)\n",
    "                val_predict = estimator.predict(X_val)\n",
    "                \n",
    "                # skip over the first year of predictions due to high error for xgb / lgbm\n",
    "                if m > 1:\n",
    "                    val_predictions = np.append(val_predictions, val_predict, axis=0)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "                # calculate and append training and validation errors\n",
    "                train_error, val_error = calc_residuals(estimator, X_train, y_train, X_val, y_val, train_error, val_error)\n",
    "            \n",
    "            # append predictions for all validation samples / models (n_samples x m_models\n",
    "            # and all errors (n_years x m_models) to dataframes \n",
    "            est_predictions = pd.concat([est_predictions, pd.Series(val_predictions, name=est)], axis=1)\n",
    "            est_errors = pd.concat([est_errors, pd.Series(val_error, name=est)], axis=1)\n",
    "        \n",
    "        # create weights based on the mean errors across all years for each model\n",
    "        est_errors = est_errors.iloc[1:, :]\n",
    "        frac = 1 - (est_errors.mean() / (est_errors.mean().sum()))\n",
    "        weights = frac / frac.sum()            \n",
    "        \n",
    "        # multiply the outputs from each model by their weights and sum to get final prediction\n",
    "        wt_results = pd.concat([df_train[df_train.year > 1].reset_index(drop=True),\n",
    "                                pd.Series((est_predictions*weights).sum(axis=1), name='pred')],\n",
    "                                axis=1)\n",
    "        wt_results['error'] = wt_results.y_act - wt_results.pred\n",
    "        \n",
    "        # calculate r_squared and correlation for n+1 results using predictions and avg_pick\n",
    "        compare_metrics = error_compare(wt_results)\n",
    "\n",
    "        # create a list of model weights and average RMSE for ensemble to append to df for export\n",
    "        wt_rmse = np.sqrt(mean_squared_error(wt_results.pred, df_train[df_train.year > 1].reset_index(drop=True).y_act))\n",
    "        wt_list = list(weights.values)\n",
    "        wt_list.append(wt_rmse)\n",
    "        wt_list.extend(compare_metrics)\n",
    "        summary = summary.append([(wt_list)])\n",
    "    \n",
    "    summary = summary.reset_index(drop=True)\n",
    "    estimators.extend(['rmse', 'rsq_pred', 'corr_pred', 'rsq_avg_pick', 'corr_avg_pick'])\n",
    "    summary.columns = estimators\n",
    "    summary = summary.sort_values(by='rmse', ascending=True)\n",
    "    \n",
    "    return param_tracker, summary, wt_results, est_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(best_result, param_list, summary, df_train, df_predict, figsize=(6,15)):\n",
    "    \n",
    "    param_list = param_list[best_result]\n",
    "    weights = summary.iloc[best_result, :len(param_list)]\n",
    "    est_names = summary.columns[:len(param_list)]\n",
    "    \n",
    "    X_train, X_val, y_train, _ = X_y_split(df_train, df_predict)\n",
    "    \n",
    "    predictions = pd.DataFrame()\n",
    "    \n",
    "    models = []\n",
    "    for est in est_names[0:len(param_list)]:\n",
    "        estimator, _ = get_estimator(est, param_list, rand=False)\n",
    "        \n",
    "        estimator.fit(X_train, y_train)\n",
    "        test_predictions = pd.Series(estimator.predict(X_val), name=est)\n",
    "        \n",
    "        predictions = pd.concat([predictions, test_predictions], axis=1)\n",
    "        models.append(estimator)\n",
    "        \n",
    "    wt_predictions = pd.Series((predictions*weights).sum(axis=1), name='pred')\n",
    "    wt_predictions = pd.concat([df_predict.reset_index(drop=True), wt_predictions], axis=1)\n",
    "    \n",
    "    to_plot = wt_predictions.pred\n",
    "    to_plot.index = wt_predictions.player\n",
    "    to_plot.sort_values().plot.barh(figsize=figsize);\n",
    "    \n",
    "    \n",
    "    return wt_predictions, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_roc(estimator, X_train, y_train, X_val, y_val, train_rmses, val_rmses, avg='macro'):\n",
    "    '''\n",
    "    input: estimator, feature set to be predicted, ground truth\n",
    "    output: RMSE value for out-of-sample predctions and list of predictions\n",
    "    '''\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "    predict_train = estimator.predict(X_train)\n",
    "    train_rmses.append(roc_auc_score(y_train, predict_train, average=avg))\n",
    "    \n",
    "    predict_val = estimator.predict(X_val)\n",
    "    val_rmses.append(roc_auc_score(y_val, predict_val, average='weighted'))\n",
    "    \n",
    "    return train_rmses, val_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_class(df_train, estimator, df_predict, scale=True, proba=False, avg='macro', pca=False):\n",
    "    '''\n",
    "    input: training dataset, estimator\n",
    "    output: out-of-sample errors and predictions for 5 timeframes\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    val_rmses = []    \n",
    "    train_rmses = []\n",
    "    train_predictions = np.array([])\n",
    "    years = df_train.year.unique()[1:]\n",
    "    \n",
    "    for i in years:\n",
    "        # create training set for all previous years and validation set for current year\n",
    "        train_split = df_train[df_train.year < i]\n",
    "        val_split = df_train[df_train.year == i]\n",
    "        \n",
    "        # splitting in X and y\n",
    "        X_train, X_val, y_train, y_val = X_y_split(train_split, val_split, scale, pca)\n",
    "        \n",
    "        # fit training data and predict validation data\n",
    "        estimator.fit(X_train, y_train)\n",
    "        if proba == True:\n",
    "            val_predict = estimator.predict_proba(X_val)[:,1]\n",
    "        else:\n",
    "            val_predict = estimator.predict(X_val)\n",
    "        train_predictions = np.append(train_predictions, val_predict, axis=0)\n",
    "        \n",
    "        # determine training and validation errors\n",
    "        train_rmses, val_rmses = predict_roc(estimator, X_train, y_train, X_val, y_val, train_rmses, val_rmses, avg)\n",
    "        \n",
    "    # create predictions for upcoming, unknown year\n",
    "    X_train, X_val, y_train, _ = X_y_split(df_train, df_predict)\n",
    "    estimator.fit(X_train, y_train)\n",
    "    if proba == True:\n",
    "        test_predictions = estimator.predict_proba(X_val)[:,1]\n",
    "    else:\n",
    "        test_predictions = estimator.predict(X_val)\n",
    "    \n",
    "    # append training and validation erros\n",
    "    train_rmses.append(np.mean(train_rmses[3:]))\n",
    "    val_rmses.append(np.mean(val_rmses[3:]))\n",
    "    \n",
    "    # printing results\n",
    "    labels = [str(year) for year in years]\n",
    "    labels.append('MEAN')\n",
    "    \n",
    "    results = pd.DataFrame([train_rmses, val_rmses]).T\n",
    "    results.columns = ['Train Error', 'Test Error']\n",
    "    results.index = labels\n",
    "    print(results)\n",
    "    \n",
    "    return estimator, train_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, label, asc=True, barh=True, figsize=(6,16), fontsize=12):\n",
    "    '''\n",
    "    Input:  The feature importance or coefficient weights from a trained model.\n",
    "    Return: A plot of the ordered weights, demonstrating relative importance of each feature.\n",
    "    '''\n",
    "    \n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #cols = df_predict.select_dtypes(include=['float', 'int', 'uint8']).columns\n",
    "    series = pd.Series(results, index=label).sort_values(ascending=asc)\n",
    "    \n",
    "    if barh == True:\n",
    "        ax = series.plot.barh(figsize=figsize, fontsize=fontsize)\n",
    "        #ax.set_xlabel(label, fontsize=fontsize+1)\n",
    "    else:\n",
    "        ax = series.plot.bar(figsize=figsize, fontsize=fontsize)\n",
    "        #ax.set_ylabel(label, fontsize=fontsize+1)\n",
    "        \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outcomes(df, outcomes, year_cutoff):\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    tmp = df[df.year > 0].reset_index(drop=True)\n",
    "    ind = tmp[tmp.year > year_cutoff].index\n",
    "    outcomes = outcomes.iloc[ind]\n",
    "    \n",
    "    df = df[df.year > year_cutoff]\n",
    "    \n",
    "    df['pred'] = outcomes.values\n",
    "    try:\n",
    "        df['error']  = df.y_act - df.pred\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error(df, year_cutoff = 0):\n",
    "    \n",
    "    from scipy.stats import pearsonr\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    df = df[df.year > year_cutoff].reset_index(drop=True)\n",
    "\n",
    "    lr = LinearRegression().fit(df.pred.values.reshape(-1,1), df.y_act)\n",
    "    r_sq = lr.score(df.pred.values.reshape(-1,1), df.y_act)\n",
    "    print('R-Squared: ', round(r_sq, 3))\n",
    "    \n",
    "    plt.scatter(df.pred, df.y_act)\n",
    "    plt.plot(range(6,20), range(6,20))\n",
    "    print('Prediction vs. Actual Correlation:', \n",
    "          round(pearsonr(df.pred, df.y_act)[0], 3))\n",
    "    plt.scatter(df.pred, df.y_act);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class clustering():\n",
    "\n",
    "    def __init__(self, df_train, df_test, model_weights, pred_weight=2):\n",
    "    \n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "        \n",
    "        # scale feature importances and take mean for weighting clustering\n",
    "        all_feature_wts = []\n",
    "        for model in model_weights:\n",
    "            try:\n",
    "                features = MinMaxScaler().fit_transform(abs(model.feature_importances_).reshape(-1,1))\n",
    "            except:\n",
    "                features = MinMaxScaler().fit_transform(abs(model.coef_).reshape(-1,1))\n",
    "                \n",
    "            all_feature_wts.append(features)\n",
    "\n",
    "        mean_features = np.mean(all_feature_wts, axis=0)[:,0]\n",
    "    \n",
    "        # create df for clustering by selecting numeric value and dropping y_act\n",
    "        self.X_train = df_train.select_dtypes(include=['float', 'int', 'uint8']).drop(['y_act', 'error'], axis=1)\n",
    "        self.X_test = df_test.select_dtypes(include=['float', 'int', 'uint8'])\n",
    "\n",
    "        # scale all columns\n",
    "        scale = StandardScaler().fit(self.X_train)\n",
    "        \n",
    "        self.X_train = scale.transform(self.X_train)\n",
    "        self.X_test = scale.transform(self.X_test)\n",
    "    \n",
    "        # weight the columns according to mean coefficients and add weight for predictions\n",
    "        self.X_train = pd.DataFrame(self.X_train * np.append(mean_features, pred_weight))\n",
    "        self.X_test = pd.DataFrame(self.X_test * np.append(mean_features, pred_weight))\n",
    "        \n",
    "        \n",
    "    def explore_k(self, k=15):\n",
    "        \n",
    "        from scipy.spatial.distance import cdist\n",
    "        from sklearn.cluster import KMeans\n",
    "        from sklearn import metrics\n",
    "\n",
    "        # k means determine k\n",
    "        X_train = self.X_train\n",
    "        distortions = []\n",
    "        silhouettes = []\n",
    "        K = range(2,k)\n",
    "        for k in K:\n",
    "            kmeanModel = KMeans(n_clusters=k, random_state=1).fit(X_train)\n",
    "            kmeanModel.fit(X_train);\n",
    "            distortions.append(sum(np.min(cdist(X_train, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X_train.shape[0]);\n",
    "            \n",
    "            silhouettes.append(metrics.silhouette_score(X_train, kmeanModel.labels_))\n",
    "                               \n",
    "        # create elbow plot\n",
    "        fig = plt.figure()\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        ax1.plot(K, distortions, 'bx-')\n",
    "        ax1.set_title(\"Distortion\");\n",
    "        \n",
    "        ax2 = fig.add_subplot(122)\n",
    "        ax2.plot(K, silhouettes, 'x-')\n",
    "        ax2.set_title(\"Silhouette Score\");\n",
    "        \n",
    "        \n",
    "    def fit_and_predict(self, k=10):\n",
    "        \n",
    "        from sklearn.cluster import KMeans\n",
    "\n",
    "        # retrain with optimal cluster \n",
    "        self.k = k\n",
    "        self.kmeans = KMeans(n_clusters=k, random_state=1).fit(self.X_train)\n",
    "        self.train_results = self.kmeans.predict(self.X_train)\n",
    "        self.test_results = self.kmeans.predict(self.X_test) \n",
    "    \n",
    "    \n",
    "    def add_clusters(self):\n",
    "        \n",
    "        self.df_train['cluster'] = self.train_results\n",
    "        self.df_test['cluster'] = self.test_results\n",
    "        \n",
    "        return self.df_train, self.df_test\n",
    "            \n",
    "    \n",
    "    def show_results(self, j):\n",
    "        from scipy.stats import pearsonr\n",
    "    \n",
    "        # calculate and print all percentiles for players in group\n",
    "        percentile = np.percentile(self.df_train[self.df_train.cluster == j].y_act, q=[5, 25, 50, 75, 95])\n",
    "        print('Fantasy PPG for Various Percentiles')\n",
    "        print('-----------------------------------')\n",
    "        print('5th percentile: ', round(percentile[0], 2))\n",
    "        print('25th percentile:', round(percentile[1], 2))\n",
    "        print('50th percentile:', round(percentile[2], 2))\n",
    "        print('75th percentile:', round(percentile[3], 2))\n",
    "        print('95th percentile:', round(percentile[4], 2))\n",
    "    \n",
    "        # show plot of historical actual results for cluster\n",
    "        ax = self.df_train[self.df_train.cluster == j].y_act.plot.hist()\n",
    "        ax.set_xlabel('Fantasy PPG Actual')\n",
    "    \n",
    "        # show plot of predicted vs actual points for cluster\n",
    "        ax = self.df_train[self.df_train.cluster == j].plot.scatter('pred', 'y_act')\n",
    "        ax.set_xlabel('Predicted Fantasy PPG')\n",
    "        ax.set_ylabel('Actual Fantasy PPG')\n",
    "        \n",
    "        # show correlation coefficient between actual and predicted points for cluster\n",
    "        print('')\n",
    "        print('Pred to Actual Correlation')\n",
    "        print(round(pearsonr(self.df_train[self.df_train.cluster==j].pred, self.df_train[self.df_train.cluster==j].y_act)[0], 3))\n",
    "        \n",
    "        # show examples of past players in cluster\n",
    "        current = self.df_test[self.df_test.cluster == j].sort_values(by='pred', ascending=True)[['player', 'avg_pick', 'pred']]\n",
    "        \n",
    "        return current\n",
    "    \n",
    "    \n",
    "    def create_distributions(self, wt=2.5):\n",
    "        from scipy.stats import skewnorm\n",
    "        from scipy.stats import pearsonr\n",
    "        import numpy as np\n",
    "\n",
    "        distributions = pd.DataFrame()\n",
    "\n",
    "        for i in range(0,self.k):\n",
    "            tmp = self.df_train[self.df_train.cluster == i]\n",
    "            \n",
    "            # caclculate mean / median, standard error, and skew of actual distribution\n",
    "            baseline = max(tmp.y_act.median(), tmp.y_act.mean())\n",
    "            std_err = tmp.y_act.std() / np.sqrt(len(tmp.y_act))\n",
    "            skew = tmp.y_act.skew()\n",
    "            \n",
    "            # calculate the difference between the center of the distribution and each player in the cluster\n",
    "            diff = self.df_test[self.df_test.cluster==i].pred - baseline\n",
    "\n",
    "            # shift players to / from the mean based on difference weighted by standard error\n",
    "            new_pred = baseline + std_err*diff\n",
    "            \n",
    "            # adjust predictions with the shifted predictions\n",
    "            self.df_test.loc[self.df_test.cluster == i, 'pred'] = new_pred\n",
    "    \n",
    "            # create error distribution based on skew, and weighted standard error\n",
    "            error_dist = skewnorm.rvs(skew, size=1000)*std_err*wt\n",
    "            distributions = distributions.append([list(error_dist)])\n",
    "    \n",
    "        distributions['cluster'] = range(0,self.k)\n",
    "        \n",
    "        rb_sampling = self.df_test[['player', 'cluster', 'pred']]\n",
    "        rb_sampling = pd.merge(rb_sampling, distributions, how='left', left_on='cluster', right_on='cluster')\n",
    "        \n",
    "        return rb_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_projections(data, player):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    df = data[data.player == player]\n",
    "    ax = pd.Series(16*(df.iloc[0][3:] + df.pred.values)).plot.hist()\n",
    "    ax.set_xlabel('Fantasy Points');\n",
    "    ax.set_title(player + ' Projections');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
