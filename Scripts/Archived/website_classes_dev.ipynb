{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============\n",
    "# Load Packages\n",
    "#==============\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# change directory temporarily to helper scripts\n",
    "os.chdir(path + 'Scripts/Analysis/Helper_Scripts')\n",
    "\n",
    "# load custom helper functions\n",
    "from helper_functions import *;\n",
    "\n",
    "\n",
    "#===============\n",
    "# Settings and User Inputs\n",
    "#===============\n",
    "\n",
    "# set core path\n",
    "path = '/Users/Mark/Documents/Github/Fantasy_Football/'\n",
    "\n",
    "# postgres login information\n",
    "pg_log = {\n",
    "    'USER': 'postgres',\n",
    "    'PASSWORD': 'Ctdim#1bf!!!!!',\n",
    "    'HOST': 'localhost',\n",
    "    'PORT': '5432', \n",
    "    'DATABASE_NAME': 'fantasyfootball'\n",
    "}\n",
    "\n",
    "# create engine for connecting to database\n",
    "engine = create_engine('postgres+psycopg2://{}:{}@{}:{}/{}'.format(pg_log['USER'], pg_log['PASSWORD'], pg_log['HOST'],\n",
    "                                                                   pg_log['PORT'], pg_log['DATABASE_NAME']))\n",
    "\n",
    "# define dictionary that contains all relevant point values\n",
    "pts_dict = {}\n",
    "pts_dict['QB'] = [0.04, 5, 0.1, 7, -2, -1]\n",
    "pts_dict['RB'] = [0.1, 0.1, 0.5, 7]\n",
    "pts_dict['WR'] = [0.1, 0.5, 7]\n",
    "pts_dict['TE'] = [0.1, 0.5, 7]\n",
    "\n",
    "# set random user id\n",
    "user_id=20\n",
    "\n",
    "# specify schema and table to write out intermediate results\n",
    "table_info = {\n",
    "    'schema': 'website',\n",
    "}\n",
    "\n",
    "# set year\n",
    "year = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in Player Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_train_data(pts_dict, pos, engine, table_info, set_year=2018):\n",
    "    \n",
    "    '''\n",
    "    The initialization of this Class reads in all of the statistical projection data and\n",
    "    translates it into clusters and projection distributions given a particular scoring schema.\n",
    "    The data is then stored in the self.data object, which will be accessed through the analysis.\n",
    "\n",
    "    Input: A database that contains statistical projections, a dictionary that contains the points\n",
    "           for each category, and number of prior repeats to use for Bayesian updating.\n",
    "    Return: Stores all the player projection distributions in that self.data object.\n",
    "    '''\n",
    "\n",
    "    # create empty dataframe to store all player distributions\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # print current position update\n",
    "    #print('Loading and Preparing ' + pos[1:] + ' Data')\n",
    "\n",
    "    #--------\n",
    "    # Connect to Database and Pull Player Data\n",
    "    #--------\n",
    "\n",
    "    df_train_results = pd.read_sql_query('SELECT * FROM {}.\"{}_Train_Results_{}\"' \\\n",
    "                                         .format(table_info['schema'], pos[1:], str(set_year)), engine)\n",
    "    df_test_results = pd.read_sql_query('SELECT * FROM {}.\"{}_Test_Results_{}\"' \\\n",
    "                                        .format(table_info['schema'], pos[1:], str(set_year)), engine)\n",
    "    df_train = pd.read_sql_query('SELECT * FROM {}.\"{}_Train_{}\"' \\\n",
    "                                 .format(table_info['schema'], pos[1:], str(set_year)), engine)\n",
    "    df_predict = pd.read_sql_query('SELECT * FROM {}.\"{}_Predict_{}\"' \\\n",
    "                                   .format(table_info['schema'], pos[1:], str(set_year)), engine)\n",
    "\n",
    "    #--------\n",
    "    # Calculate Fantasy Points for Given Scoring System and Cluster\n",
    "    #--------\n",
    "\n",
    "    # pull in data results from dataframe\n",
    "    df_train_results, df_test_results = format_results(df_train_results, df_test_results, \n",
    "                                                       df_train, df_predict, \n",
    "                                                       pts_dict[pos[1:]])\n",
    "    #df_train_results = df_train_results.drop('year', axis=1)\n",
    "\n",
    "    return df_train_results, df_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringDev():\n",
    "    \n",
    "    def __init__(self, df_train, df_test):\n",
    "    \n",
    "        import pandas as pd\n",
    "        \n",
    "        # create self versions of train and test\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "    \n",
    "        # create df for clustering by selecting numeric values and dropping y_act\n",
    "        self.X_train = df_train.select_dtypes(include=['float', 'int', 'uint8']).drop(['y_act', 'error'], axis=1)\n",
    "        #self.X_test = df_test.select_dtypes(include=['float', 'int', 'uint8']).drop([], axis=1)\n",
    "        self.X_test = df_test.select_dtypes(include=['float', 'int', 'uint8']).drop(['y_act', 'error'], axis=1)\n",
    "        self.y = df_train.y_act\n",
    "    \n",
    "        #=========\n",
    "        # Set the RF search params for each position\n",
    "        #=========\n",
    "\n",
    "        self.pos = {'QB': {}, 'RB': {}, 'WR': {}, 'TE': {}}\n",
    "\n",
    "        self.pos['QB']['tree_params'] = {\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [15, 20, 25],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "        self.pos['RB']['tree_params'] = {\n",
    "            'max_depth': [5, 6, 7],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [15, 20, 25],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "        self.pos['WR']['tree_params'] = {\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [20, 25, 30],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "\n",
    "        self.pos['TE']['tree_params'] = {\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [15, 20, 25],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _searching(est, params, X_grid, y_grid, n_jobs=1, print_results=True):\n",
    "        '''\n",
    "        Function to perform GridSearchCV and return the test RMSE, as well as the \n",
    "        optimized and fitted model\n",
    "        '''\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        Search = GridSearchCV(estimator=est,\n",
    "                              param_grid=params,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=n_jobs,\n",
    "                              cv=3,\n",
    "                              return_train_score=True,\n",
    "                              iid=False)\n",
    "\n",
    "        search_results = Search.fit(X_grid, y_grid)\n",
    "\n",
    "        best_params = search_results.cv_results_['params'][search_results.best_index_]\n",
    "        est.set_params(**best_params)\n",
    "\n",
    "        est.fit(X_grid, y_grid)\n",
    "\n",
    "        return est\n",
    "\n",
    "\n",
    "        \n",
    "    def fit_and_predict_tree(self, pos, print_results=False):\n",
    "        \n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        \n",
    "        dtree = self._searching(DecisionTreeRegressor(random_state=1), self.pos[pos]['tree_params'], \n",
    "                               self.X_train, self.y, print_results=print_results)\n",
    "        \n",
    "        #----------\n",
    "        # Calculate each cluster's mean and standard deviation\n",
    "        #----------\n",
    "\n",
    "        # pull out the training clusters and cbind with the actual points scored\n",
    "        train_results = pd.concat([pd.Series(dtree.apply(self.X_train), name='Cluster'), self.y], axis=1)\n",
    "\n",
    "        # calculate the average and standard deviation of points scored by cluster\n",
    "        train_results = train_results.groupby('Cluster', as_index=False).agg({'y_act': ['mean', 'std']})\n",
    "        train_results.columns = ['Cluster', 'ClusterMean', 'ClusterStd']\n",
    "\n",
    "        #----------\n",
    "        # Add the cluster to test results and resulting group mean / std: Player | Pred | StdDev\n",
    "        #----------\n",
    "\n",
    "        # grab the player, prediction, and add cluster to dataset\n",
    "        test_results = pd.concat([self.df_test[['player', 'pred']], \n",
    "                                  pd.Series(dtree.apply(self.X_test), name='Cluster')], axis=1)\n",
    "\n",
    "        # merge the test results with the train result on cluster to add mean cluster and std\n",
    "        test_results = pd.merge(test_results, train_results, how='inner', left_on='Cluster', right_on='Cluster')\n",
    "\n",
    "        # calculate an overall prediction mean\n",
    "        test_results['PredMean'] = (0.5*test_results.pred + 0.5*test_results.ClusterMean)\n",
    "        \n",
    "        #test_results = test_results[['player', 'PredMean', 'ClusterStd']]\n",
    "        test_results = test_results[['player','pred', 'ClusterMean', 'PredMean', 'ClusterStd']]\n",
    "        \n",
    "        return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for pos in ['aQB', 'bRB', 'cWR', 'dTE']:\n",
    "\n",
    "    train_results, test_results = rf_train_data(pts_dict, pos, engine, table_info)\n",
    "    \n",
    "    cluster_dev = ClusteringDev(train_results, test_results)\n",
    "    dist_input = cluster_dev.fit_and_predict_tree(pos[1:])\n",
    "time.time()-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_year = 2016\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create object to hold results\n",
    "all_data = pd.DataFrame()\n",
    "all_results = []\n",
    "\n",
    "# loop through each test year\n",
    "for test_year in [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]:\n",
    "    pos = 'aTE'\n",
    "    \n",
    "    # pull out the train and test datasets and create temp tables based on current year\n",
    "    train_results, test_results = rf_train_data(pts_dict, pos, engine, table_info)\n",
    "    train_tmp = train_results[train_results.year < test_year].drop('year', axis=1).reset_index(drop=True)\n",
    "    test_tmp = train_results[train_results.year==test_year].drop('year', axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # run the clustering algorithm\n",
    "    cluster_dev = ClusteringDev(train_tmp, test_tmp)\n",
    "    dist_input = cluster_dev.fit_and_predict_tree(pos[1:])\n",
    "    \n",
    "    # paste actual values back to tes table and calculate error metrics\n",
    "    dist_input = pd.merge(dist_input, test_tmp[['player', 'y_act']], how='inner', left_on='player', right_on='player')\n",
    "    cluster_error = mean_squared_error(dist_input.ClusterMean, dist_input.y_act)\n",
    "    pred_error = mean_squared_error(dist_input.pred, dist_input.y_act)\n",
    "    combined_error = mean_squared_error(dist_input.PredMean, dist_input.y_act)\n",
    "    \n",
    "    # concatenate results\n",
    "    all_data=pd.concat([all_data, dist_input], axis=0)\n",
    "    all_results.append([np.sqrt(cluster_error), np.sqrt(pred_error), np.sqrt(combined_error)])\n",
    "\n",
    "# pull out final results into a dataframe for error metrics\n",
    "all_results = pd.DataFrame(all_results, columns=['ClusterError', 'PredError', 'CombinedError'])\n",
    "    \n",
    "# for i in test_results.iterrows():\n",
    "#     x=np.random.normal(loc=i[1]['PredMean'], scale=i[1]['ClusterStd'], size=1000)*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringBayes():\n",
    "    \n",
    "    def __init__(self, df_train, df_test):\n",
    "    \n",
    "        import pandas as pd\n",
    "        \n",
    "        # create self versions of train and test\n",
    "        self.df_train = df_train\n",
    "        self.df_test = df_test\n",
    "    \n",
    "        # create df for clustering by selecting numeric values and dropping y_act\n",
    "        self.X_train = df_train.select_dtypes(include=['float', 'int', 'uint8']).drop(['y_act', 'error'], axis=1)\n",
    "        #self.X_test = df_test.select_dtypes(include=['float', 'int', 'uint8']).drop([], axis=1)\n",
    "        self.X_test = df_test.select_dtypes(include=['float', 'int', 'uint8']).drop(['y_act', 'error'], axis=1)\n",
    "        self.y = df_train.y_act\n",
    "    \n",
    "        #=========\n",
    "        # Set the RF search params for each position\n",
    "        #=========\n",
    "\n",
    "        self.pos = {'QB': {}, 'RB': {}, 'WR': {}, 'TE': {}}\n",
    "\n",
    "        self.pos['QB']['tree_params'] = {\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [15, 20, 25],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "        self.pos['RB']['tree_params'] = {\n",
    "            'max_depth': [5, 6, 7],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [15, 20, 25],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "        self.pos['WR']['tree_params'] = {\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [20, 25, 30],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "\n",
    "        self.pos['TE']['tree_params'] = {\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [15, 20, 25],\n",
    "            'splitter': ['random']\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _searching(est, params, X_grid, y_grid, n_jobs=1, print_results=True):\n",
    "        '''\n",
    "        Function to perform GridSearchCV and return the test RMSE, as well as the \n",
    "        optimized and fitted model\n",
    "        '''\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "\n",
    "        Search = GridSearchCV(estimator=est,\n",
    "                              param_grid=params,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              n_jobs=n_jobs,\n",
    "                              cv=3,\n",
    "                              return_train_score=True,\n",
    "                              iid=False)\n",
    "\n",
    "        search_results = Search.fit(X_grid, y_grid)\n",
    "\n",
    "        best_params = search_results.cv_results_['params'][search_results.best_index_]\n",
    "        est.set_params(**best_params)\n",
    "\n",
    "        est.fit(X_grid, y_grid)\n",
    "\n",
    "        return est\n",
    "\n",
    "\n",
    "        \n",
    "    def fit_and_predict_tree(self, pos, print_results=False):\n",
    "        \n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        \n",
    "        dtree = self._searching(DecisionTreeRegressor(random_state=1), self.pos[pos]['tree_params'], \n",
    "                                self.X_train, self.y, print_results=print_results)\n",
    "        \n",
    "        #----------\n",
    "        # Calculate each cluster's mean and standard deviation\n",
    "        #----------\n",
    "\n",
    "        # pull out the training clusters and cbind with the actual points scored\n",
    "        train_results = pd.concat([self.df_train[['player', 'pred']],\n",
    "                                   pd.Series(dtree.apply(self.X_train), name='cluster'), \n",
    "                                   self.y], axis=1)\n",
    "\n",
    "        #----------\n",
    "        # Add the cluster to test results and resulting group mean / std: Player | Pred | StdDev\n",
    "        #----------\n",
    "\n",
    "        # grab the player, prediction, and add cluster to dataset\n",
    "        test_results = pd.concat([self.df_test[['player', 'pred']], \n",
    "                                  pd.Series(dtree.apply(self.X_test), name='cluster')], axis=1)\n",
    "        \n",
    "        return train_results, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distributions(df_train, df_test, prior_repeats=15, dist_size=1000, show_plots=False):\n",
    "\n",
    "    # historical standard deviation and mean for actual results\n",
    "    hist_std = df_train.groupby('player').agg('std').dropna()\n",
    "    hist_mean = df_train.groupby('player').agg('mean').dropna()\n",
    "\n",
    "    # merge historicaly mean and standard deviations\n",
    "    hist_mean_std = pd.merge(hist_std, hist_mean, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    # calculate global coefficient of variance for players that don't have enough historical results\n",
    "    global_cv = (hist_mean_std.y_act_x / hist_mean_std.y_act_y).mean()\n",
    "\n",
    "    #==========\n",
    "    # Loop to Create Prior and Posterior Distributions\n",
    "    #==========\n",
    "\n",
    "    df_test = df_test.sort_values(by='pred', ascending=False)\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    for player in df_test.player[0:]:\n",
    "\n",
    "        # set seed\n",
    "        np.random.seed(1234)\n",
    "\n",
    "        # create list for results\n",
    "        results_list = [player]\n",
    "\n",
    "        #==========\n",
    "        # Pull Out Predictions and Actual Results for Given Player to Create Prior\n",
    "        #==========\n",
    "\n",
    "        #--------\n",
    "        # Extract this year's results and multiply by prior_repeats\n",
    "        #--------\n",
    "\n",
    "        # extract predictions from ensemble and updated predictions based on cluster fit\n",
    "        ty = df_test.loc[df_test.player == player, ['player', 'pred']]\n",
    "        #ty_c = self.df_test.loc[self.df_test.player == player, ['player', 'cluster_pred']]\n",
    "\n",
    "        # replicate the predictions to increase n_0 for the prior\n",
    "        ty = pd.concat([ty]*prior_repeats, ignore_index=True)\n",
    "        #ty_c = pd.concat([ty_c]*prior_repeats, ignore_index=True)\n",
    "\n",
    "        # rename the prediction columns to 'points'\n",
    "        ty = ty.rename(columns={'pred': 'points'})\n",
    "        #ty_c = ty_c.rename(columns={'cluster_pred': 'points'})\n",
    "\n",
    "        #--------\n",
    "        # Extract previous year's results, if available\n",
    "        #--------\n",
    "\n",
    "        # pull out the most recent 5 years worth of performance, if available\n",
    "        py = df_train.loc[df_train.player == player, ['player', 'y_act']].reset_index(drop=True)[0:5]\n",
    "\n",
    "        # convert y_act to points name\n",
    "        py = py.rename(columns={'y_act': 'points'})\n",
    "\n",
    "        #--------\n",
    "        # Create Prior Distribution and Conjugant Hyperparameters\n",
    "        #--------\n",
    "\n",
    "        # combine this year's prediction, the cluster prediction, and previous year actual, if available\n",
    "        priors = pd.concat([ty, py], axis=0)\n",
    "\n",
    "        # set m_0 to the priors mean\n",
    "        m_0 = priors.points.mean()\n",
    "\n",
    "        # Create the prior variance through a weighted average of the actual previous year\n",
    "        # performance and a global coefficient of variance multiple by the prior mean.\n",
    "        # If there is not at least 3 years of previous data, simply use the global cv.\n",
    "        if py.shape[0] >= 3:\n",
    "            s2_0 = ((py.shape[0]*py.points.std()**2) + (2*prior_repeats*(m_0 * global_cv)**2)) / (py.shape[0] + 2*prior_repeats)\n",
    "        else:\n",
    "            s2_0 = (m_0 * global_cv)**2\n",
    "\n",
    "        # set the prior sample size and degrees of freedom\n",
    "        n_0 = priors.shape[0]\n",
    "        v_0 = n_0 - 1\n",
    "\n",
    "        # calculate the prior distribution\n",
    "        prior_y = np.random.normal(loc=m_0, scale=np.sqrt(s2_0), size=dist_size)\n",
    "\n",
    "        #--------\n",
    "        # Create the Data and Data Hyperparameters\n",
    "        #--------\n",
    "\n",
    "        # pull out the cluster for the current player\n",
    "        ty_cluster = df_test[df_test.player == player].cluster.values[0]\n",
    "\n",
    "        # create a list of the actual points scored to be used as updating data\n",
    "        update_data = df_train[df_train.cluster == ty_cluster].y_act\n",
    "\n",
    "        # set ybar to the mean of the update data\n",
    "        ybar = update_data.mean()\n",
    "\n",
    "        # calculate the standard deviation based on the 5th and 95th percentiles\n",
    "        s2 = ((np.percentile(update_data, q=95)-np.percentile(update_data, q=5)) / 4.0)**2\n",
    "\n",
    "        # determine the n as the number of data points\n",
    "        n = len(update_data)\n",
    "\n",
    "        #--------\n",
    "        # Create the Posterior Distribution \n",
    "        #--------\n",
    "\n",
    "        # set the poster n samples\n",
    "        n_n = n_0 + n \n",
    "\n",
    "        # update the poster mean\n",
    "        m_n = (n*ybar + n_0*m_0) / n_n \n",
    "\n",
    "#         # update the posterior degrees of freedom\n",
    "#         v_n = v_0 + n \n",
    "\n",
    "#         # update the posterior variance\n",
    "#         s2_n = ((n-1)*s2 + v_0*s2_0 + (n_0*n*(m_0 - ybar)**2)/n_n)/v_n\n",
    "\n",
    "#         # calculate the gamma distribution and convert to sigma\n",
    "#         phi = np.random.gamma(shape=v_n/2, scale=2/(s2_n*v_n), size=dist_size)\n",
    "#         sigma = 1/np.sqrt(phi)\n",
    "\n",
    "#         # calculate the posterior mean\n",
    "#         post_mu = np.random.normal(loc=m_n, scale=sigma/(np.sqrt(n_n)), size=dist_size)\n",
    "\n",
    "#         # create the posterior distribution\n",
    "#         pred_y =  np.random.normal(loc=post_mu, scale=sigma, size=dist_size)\n",
    "\n",
    "        results_list.append(m_n)\n",
    "        results = pd.concat([results, pd.DataFrame(results_list).T], axis=0)\n",
    "\n",
    "    return results.reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_errors = []\n",
    "\n",
    "for test_year in [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]:\n",
    "#for pos in ['aQB', 'aRB', 'aWR', 'aTE']:\n",
    "    \n",
    "    # pull out the train and test datasets and create temp tables based on current year\n",
    "    train_results, test_results = rf_train_data(pts_dict, pos, engine, table_info)\n",
    "    train_tmp = train_results[train_results.year < test_year].drop('year', axis=1).reset_index(drop=True)\n",
    "    test_tmp = train_results[train_results.year==test_year].drop('year', axis=1).reset_index(drop=True)\n",
    "\n",
    "    # run the clustering algorithm\n",
    "    cluster_dev = ClusteringBayes(train_tmp, test_tmp)\n",
    "    train_bayes, test_bayes = cluster_dev.fit_and_predict_tree(pos[1:])\n",
    "\n",
    "    bayes_output = create_distributions(train_bayes, test_bayes)\n",
    "    bayes_output = pd.merge(bayes_output, test_tmp, how='inner', left_on=0, right_on='player')\n",
    "\n",
    "    bayes_errors.append(np.sqrt(mean_squared_error(bayes_output.iloc[:,1], bayes_output.y_act)))\n",
    "\n",
    "all_results = pd.concat([all_results, pd.Series(bayes_errors, name='BayesError')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
