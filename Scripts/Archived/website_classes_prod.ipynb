{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============\n",
    "# Load Packages\n",
    "#==============\n",
    "\n",
    "# set core path\n",
    "path = '/Users/Mark/Documents/Github/Fantasy_Football/'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# change directory temporarily to helper scripts\n",
    "os.chdir(path + 'Scripts/Analysis/Helper_Scripts')\n",
    "\n",
    "# load custom helper functions\n",
    "from helper_functions import *;\n",
    "\n",
    "\n",
    "#===============\n",
    "# Settings and User Inputs\n",
    "#===============\n",
    "\n",
    "#--------\n",
    "# League Settings\n",
    "#--------\n",
    "\n",
    "# define point values for all statistical categories\n",
    "pass_yd_per_pt = 0.04 \n",
    "pass_td_pt = 5\n",
    "int_pts = -2\n",
    "sacks = -1\n",
    "rush_yd_per_pt = 0.1 \n",
    "rec_yd_per_pt = 0.1\n",
    "rush_rec_td = 7\n",
    "ppr = 0.5\n",
    "\n",
    "# creating dictionary containing point values for each position\n",
    "pts_dict = {}\n",
    "pts_dict['QB'] = [pass_yd_per_pt, pass_td_pt, rush_yd_per_pt, rush_rec_td, int_pts, sacks]\n",
    "pts_dict['RB'] = [rush_yd_per_pt, rec_yd_per_pt, ppr, rush_rec_td]\n",
    "pts_dict['WR'] = [rec_yd_per_pt, ppr, rush_rec_td]\n",
    "pts_dict['TE'] = [rec_yd_per_pt, ppr, rush_rec_td]\n",
    "\n",
    "#--------\n",
    "# Database Login Info\n",
    "#--------\n",
    "\n",
    "# postgres login information\n",
    "pg_log = {\n",
    "    'USER': 'postgres',\n",
    "    'PASSWORD': 'Ctdim#1bf!!!!!',\n",
    "    'HOST': 'localhost',\n",
    "    'PORT': '5432', \n",
    "    'DATABASE_NAME': 'fantasyfootball'\n",
    "}\n",
    "\n",
    "# create engine for connecting to database\n",
    "engine = create_engine('postgres+psycopg2://{}:{}@{}:{}/{}'.format(pg_log['USER'], pg_log['PASSWORD'], pg_log['HOST'],\n",
    "                                                                   pg_log['PORT'], pg_log['DATABASE_NAME']))\n",
    "\n",
    "# specify schema and table to write out intermediate results\n",
    "table_info = {\n",
    "    'engine': engine,\n",
    "    'schema': 'websitedev',\n",
    "}\n",
    "\n",
    "# set year\n",
    "year = 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling in Player Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(pts_dict, pos, table_info, set_year=2018):\n",
    "    \n",
    "    '''\n",
    "    This function reads in all raw statistical predictions from the ensemble model for a given\n",
    "    position group and then converts it into predicted points scored based on a given scoring system.\n",
    "\n",
    "    Input: Database connection to pull stored raw statistical data, a dictionary containing points\n",
    "           per statistical category, and a position to pull.\n",
    "    Return: A dataframe with a player, their raw statistical projections and the predicted points\n",
    "            scored for a given scoring system.\n",
    "    '''\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    #--------\n",
    "    # Connect to Database and Pull Player Data\n",
    "    #--------\n",
    "\n",
    "    train = pd.read_sql_query('SELECT * FROM {}.\"{}_Train_{}\"' \\\n",
    "                                         .format(table_info['schema'], pos[1:], str(set_year)), table_info['engine'])\n",
    "    test = pd.read_sql_query('SELECT * FROM {}.\"{}_Test_{}\"' \\\n",
    "                                        .format(table_info['schema'], pos[1:], str(set_year)), table_info['engine'])\n",
    "\n",
    "    #--------\n",
    "    # Calculate Fantasy Points for Given Scoring System\n",
    "    #-------- \n",
    "    \n",
    "    # extract points list and get the idx of point attributes based on length of list\n",
    "    pts_list = pts_dict[pos[1:]]\n",
    "    c_idx = len(pts_list) + 1\n",
    "\n",
    "    # multiply stat categories by corresponding point values\n",
    "    train.iloc[:, 1:c_idx] = train.iloc[:, 1:c_idx] * pts_list\n",
    "    test.iloc[:, 1:c_idx] = test.iloc[:, 1:c_idx] * pts_list\n",
    "\n",
    "    # add a total predicted points stat category\n",
    "    train.loc[:, 'pred'] = train.iloc[:, 1:c_idx].sum(axis=1)\n",
    "    test.loc[:, 'pred'] = test.iloc[:, 1:c_idx].sum(axis=1)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def _searching(est, pos, X_grid, y_grid, n_jobs=1):\n",
    "    '''\n",
    "    Function to perform GridSearchCV and return the test RMSE, as well as the \n",
    "    optimized and fitted model\n",
    "    '''\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "     #=========\n",
    "    # Set the RF search params for each position\n",
    "    #=========\n",
    "\n",
    "    params = {}\n",
    "\n",
    "    params['QB'] = {\n",
    "        'max_depth': [4, 5],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [15, 20, 25],\n",
    "        'splitter': ['random']\n",
    "    }\n",
    "\n",
    "    params['RB'] = {\n",
    "        'max_depth': [5, 6, 7],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [15, 20, 25],\n",
    "        'splitter': ['random']\n",
    "    }\n",
    "\n",
    "    params['WR'] = {\n",
    "        'max_depth': [4, 5, 6],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [20, 25, 30],\n",
    "        'splitter': ['random']\n",
    "    }\n",
    "\n",
    "\n",
    "    params['TE'] = {\n",
    "        'max_depth': [4, 5],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [15, 20, 25],\n",
    "        'splitter': ['random']\n",
    "    }\n",
    "\n",
    "    # set up GridSearch object\n",
    "    Search = GridSearchCV(estimator=est,\n",
    "                          param_grid=params[pos[1:]],\n",
    "                          scoring='neg_mean_squared_error',\n",
    "                          n_jobs=n_jobs,\n",
    "                          cv=3,\n",
    "                          return_train_score=False,\n",
    "                          iid=False)\n",
    "\n",
    "    # try all combination of parameters with the fit\n",
    "    search_results = Search.fit(X_grid, y_grid)\n",
    "\n",
    "    # extract best estimator parameters and create model object with them\n",
    "    best_params = search_results.cv_results_['params'][search_results.best_index_]\n",
    "    est.set_params(**best_params)\n",
    "\n",
    "    # fit the optimal estimator with the data\n",
    "    est.fit(X_grid, y_grid)\n",
    "\n",
    "    return est\n",
    "\n",
    "\n",
    "def tree_cluster(train, test, pos):\n",
    "    \n",
    "    # create df for clustering by selecting numeric values and dropping y_act\n",
    "    X_train = train.select_dtypes(include=['float', 'int', 'uint8']).drop('y_act', axis=1)\n",
    "    X_test = test.select_dtypes(include=['float', 'int', 'uint8'])\n",
    "    y = train.y_act\n",
    "    \n",
    "    #----------\n",
    "    # Train the Decision Tree with GridSearch optimization\n",
    "    #----------\n",
    "\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "    # train decision tree with _searching method\n",
    "    dtree = _searching(DecisionTreeRegressor(random_state=1), pos, X_train, y)\n",
    "\n",
    "    #----------\n",
    "    # Calculate each cluster's mean and standard deviation\n",
    "    #----------\n",
    "\n",
    "    # pull out the training clusters and cbind with the actual points scored\n",
    "    train_results = pd.concat([pd.Series(dtree.apply(X_train), name='Cluster'), y], axis=1)\n",
    "\n",
    "    # calculate the average and standard deviation of points scored by cluster\n",
    "    train_results = train_results.groupby('Cluster', as_index=False).agg({'y_act': ['mean', 'std']})\n",
    "    train_results.columns = ['Cluster', 'ClusterMean', 'ClusterStd']\n",
    "\n",
    "    #----------\n",
    "    # Add the cluster to test results and resulting group mean / std: Player | Pred | StdDev\n",
    "    #----------\n",
    "\n",
    "    # grab the player, prediction, and add cluster to dataset\n",
    "    test_results = pd.concat([test[['player', 'pred']], \n",
    "                              pd.Series(dtree.apply(X_test), name='Cluster')], axis=1)\n",
    "\n",
    "    # merge the test results with the train result on cluster to add mean cluster and std\n",
    "    test_results = pd.merge(test_results, train_results, how='inner', left_on='Cluster', right_on='Cluster')\n",
    "\n",
    "    # calculate an overall prediction mean and add position to dataset\n",
    "    test_results['PredMean'] = (0.5*test_results.pred + 0.5*test_results.ClusterMean)\n",
    "    test_results['pos'] = pos\n",
    "\n",
    "    # pull out relevant results for creating distributions\n",
    "    test_results = test_results[['player', 'pos', 'PredMean', 'ClusterStd']]\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "\n",
    "def create_distributions():\n",
    "    \n",
    "    # create empty dataframe to store player point distributions\n",
    "    tree_output = pd.DataFrame()\n",
    "    for pos in ['aQB', 'bRB', 'cWR', 'dTE']:\n",
    "        \n",
    "        # extract the train and test data for passing into the tree algorithm\n",
    "        train, test = data_load(pts_dict, pos, table_info, set_year=2018)\n",
    "        \n",
    "        # obtain the cluster standard deviation + the mixed prediction / cluster mean\n",
    "        results = tree_cluster(train, test, pos)\n",
    "        \n",
    "        # append the results for each position into single dataframe\n",
    "        tree_output = pd.concat([tree_output, results], axis=0)\n",
    "\n",
    "    tree_output = tree_output.reset_index(drop=True)\n",
    "    \n",
    "    # loop through each row in tree output and create a normal distribution\n",
    "    data = []\n",
    "    for row in tree_output.iterrows():\n",
    "        dist = list(np.uint16(np.random.normal(loc=row[1]['PredMean'], scale=row[1]['ClusterStd'], size=1500)*16))\n",
    "        data.append(dist)\n",
    "\n",
    "    # create the player, position, point distribution dataframe\n",
    "    data = pd.concat([tree_output.player, pd.DataFrame(data), tree_output.pos], axis=1)\n",
    "    \n",
    "    # add salaries to the dataframe and set index to player\n",
    "    salaries = pd.read_sql_query('SELECT * FROM {}.\"salaries\"'.format(table_info['schema']), table_info['engine'])\n",
    "    data = pd.merge(data, salaries, how='inner', left_on='player', right_on='player')\n",
    "    data = data.set_index('player')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit x = create_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
